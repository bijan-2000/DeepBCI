{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42992946",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298368a0-a454-482f-b6b0-700c529d8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f45a8f-2a1b-4d66-a34e-20bdd9d26129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af32dba6-d557-4f5c-b2a9-6524cdd3636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\n",
    "from mne.decoding import CSP\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import copy\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#import torch\n",
    "#from torch import nn\n",
    "from sklearn.metrics import classification_report\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import random\n",
    "#from torchsummary import summary\n",
    "#import torchinfo\n",
    "#import moabb\n",
    "\n",
    "# EEGNet-specific imports\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "# Parameter tunning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# ATCNet Model imports\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa17ce7-4e1c-4f52-8963-abd86084b662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/bijan/py3x/Code_Zhang/Transfer_Learning_On_EEG_BCI']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projects/def-b09sdp/bijan/Phase2/P16.fdt\n",
    "path = !pwd\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7bc34-24a6-4250-89bf-4bc9422e0af4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec218e4-dc89-43dd-9105-6875b25f7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper(data_epochs, data_labels, mode='binary'):\n",
    "    \n",
    "    if mode == 'binary':\n",
    "        epochs = []\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(data_epochs)):\n",
    "            participant_epochs = data_epochs[i]\n",
    "            participant_labels = data_labels[i]\n",
    "    \n",
    "            binary_epochs = participant_epochs[(participant_labels==1) | (participant_labels==2)]\n",
    "            #class2_epochs = participant_epochs[participant_labels==2]\n",
    "            #bi_epochs = np.concatenate((class1_epochs, class2_epochs), axis=0)\n",
    "            epochs.append(binary_epochs)\n",
    "    \n",
    "            binary_labels = participant_labels[(participant_labels==1) | (participant_labels==2)]\n",
    "            #bi_labels = np.concatenate((class1_labels, class2_labels), axis=0)\n",
    "            labels.append(binary_labels)\n",
    "            \n",
    "    elif mode == '3_class':\n",
    "        \n",
    "        epochs = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(data_epochs)):\n",
    "            participant_epochs = data_epochs[i]\n",
    "            participant_labels = data_labels[i]\n",
    "    \n",
    "            multiclass_epochs = participant_epochs[(participant_labels==1) | (participant_labels==2) | (participant_labels==3)]\n",
    "            epochs.append(multiclass_epochs)\n",
    "    \n",
    "            multiclass_labels = participant_labels[(participant_labels==1) | (participant_labels==2) | (participant_labels==3)]\n",
    "            labels.append(multiclass_labels)\n",
    "            \n",
    "            \n",
    "    elif mode == '4_class_RS':\n",
    "        \n",
    "        epochs = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(data_epochs)):\n",
    "            participant_epochs = data_epochs[i]\n",
    "            participant_labels = data_labels[i]\n",
    "    \n",
    "            multiclass_epochs = participant_epochs[(participant_labels==2) | (participant_labels==6) | (participant_labels==5) | (participant_labels==1)]\n",
    "            epochs.append(multiclass_epochs)\n",
    "    \n",
    "            multiclass_labels = participant_labels[(participant_labels==2) | (participant_labels==6) | (participant_labels==5) | (participant_labels==1)]\n",
    "            labels.append(multiclass_labels)\n",
    "        \n",
    "    elif mode == '4_class_LS':\n",
    "        \n",
    "        epochs = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(data_epochs)):\n",
    "            participant_epochs = data_epochs[i]\n",
    "            participant_labels = data_labels[i]\n",
    "    \n",
    "            multiclass_epochs = participant_epochs[(participant_labels==3) | (participant_labels==7) | (participant_labels==5) | (participant_labels==1)]\n",
    "            epochs.append(multiclass_epochs)\n",
    "    \n",
    "            multiclass_labels = participant_labels[(participant_labels==3) | (participant_labels==7) | (participant_labels==5) | (participant_labels==1)]\n",
    "            labels.append(multiclass_labels)\n",
    "    elif mode == '6_class':\n",
    "        \n",
    "        epochs = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(len(data_epochs)):\n",
    "            participant_epochs = data_epochs[i]\n",
    "            participant_labels = data_labels[i]\n",
    "    \n",
    "            multiclass_epochs = participant_epochs[(participant_labels==3) | (participant_labels==7) | (participant_labels==5) | (participant_labels==6) | (participant_labels==2) | (participant_labels==1)]\n",
    "            epochs.append(multiclass_epochs)\n",
    "    \n",
    "            multiclass_labels = participant_labels[(participant_labels==3) | (participant_labels==7) | (participant_labels==5) | (participant_labels==6) | (participant_labels==2) | (participant_labels==1)]\n",
    "            labels.append(multiclass_labels)\n",
    "    \n",
    "    return epochs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e97a6c-2926-493a-9a07-3d750b25f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_label_extractor(data, events, epoch_length, num_channels, sampling_freq):\n",
    "    \n",
    "    data = data.to_data_frame()\n",
    "    events = events[0]\n",
    "    third_column = events[:, 2]\n",
    "    mask = np.isin(third_column, [7, 8, 9, 10])\n",
    "    MI_events = events[mask]\n",
    "    \n",
    "    number_of_epochs = MI_events.shape[0]\n",
    "    labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "    epochs = np.zeros((number_of_epochs, num_channels, epoch_length * sampling_freq))\n",
    "    index = 0\n",
    "    for index in range(number_of_epochs):\n",
    "        start = int(MI_events[index, 0])\n",
    "        end = int(MI_events[index, 0]) + epoch_length * sampling_freq\n",
    "        all_channels = data.iloc[start:end]\n",
    "        epochs[index,:,:] = all_channels[all_channels.columns[1: num_channels+1]].T\n",
    "        \n",
    "        # Because it is numbered form 7 to 10 !!!\n",
    "        labels[index] = MI_events[index, 2] - 7\n",
    "\n",
    "            \n",
    "    return epochs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f68789b-5a8b-4912-ab37-57a04047dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_label_extractor(Data, epoch_length=1123, num_channels=64):\n",
    "    df = Data.to_data_frame()\n",
    "    X = df[df.columns[3:]].to_numpy()\n",
    "    X = np.transpose(X)\n",
    "\n",
    "    number_of_epochs = int(len(df)/epoch_length)\n",
    "    \n",
    "    randomlist = random.sample(range(number_of_epochs), number_of_epochs)\n",
    "\n",
    "    data = np.zeros((number_of_epochs,num_channels, epoch_length))\n",
    "    labels = np.zeros((number_of_epochs,1)).astype(int)\n",
    "\n",
    "    for i in range(number_of_epochs):\n",
    "        data[i,:,:] = X[:, randomlist[i]*epoch_length:(randomlist[i] + 1)*epoch_length]\n",
    "        if (df['condition'][randomlist[i]*epoch_length] == 'Left'):\n",
    "            labels[i,0] = 0\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Right'):\n",
    "            labels[i,0] = 1\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Feet'):\n",
    "            labels[i,0] = 2\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Tongue'):\n",
    "            labels[i,0] = 3\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Mis'):\n",
    "            labels[i,0] = 4\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Si'):\n",
    "            labels[i,0] = 5\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Rest'):\n",
    "            labels[i,0] = 6\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Ls'):\n",
    "            labels[i,0] = 7\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Rs'):\n",
    "            labels[i,0] = 8\n",
    "        elif(df['condition'][randomlist[i]*epoch_length] == 'Fs'):\n",
    "            labels[i,0] = 9\n",
    "        else:\n",
    "            labels[i,0] = 10\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c5154e2-0354-4913-a796-40e515129978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(y_data, method=OneHotEncoder):\n",
    "    \n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(y_data[0].reshape(-1, 1))\n",
    "    \n",
    "    for i in range(len(y_data)):\n",
    "        \n",
    "        a = encoder.transform(y_data[i].reshape(-1, 1))\n",
    "        y_data[i] = a.toarray()\n",
    "\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329b65be-1f55-46c0-9302-f200d265df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_extraction(classes, data, labels):\n",
    "    mask = np.isin(labels[:, 0], classes)\n",
    "    dataset = data[mask, :, :]\n",
    "    Final_labels = labels[mask, :]\n",
    "    return dataset, Final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda8305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_csp(x_train, y_train, x_test, n_components=16):\n",
    "    \n",
    "        csp = CSP(n_components)\n",
    "        csp_fit = csp.fit(x_train, y_train)\n",
    "        train_feat = csp_fit.transform(x_train)\n",
    "        test_feat = csp_fit.transform(x_test)\n",
    "        return train_feat, test_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70687bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_4(train_data, train_labels, test_data, sampling_freq, n_components):\n",
    "\n",
    "    # Why number of bands were set to 24??\n",
    "    [n_epochs_train, n_split, n_channels, n_samples] = train_data.shape\n",
    "    [n_epochs_test, _, _, _] = test_data.shape\n",
    "    \n",
    "    \n",
    "    low_cutoff = 4\n",
    "    number_of_bands = 16\n",
    "    for b in range(number_of_bands):\n",
    "        data = train_data.copy()\n",
    "        new_train_labels = np.repeat(train_labels, n_split)\n",
    "        data = data.reshape((n_epochs_train * n_split, n_channels, n_samples))\n",
    "        data_test = test_data.copy()\n",
    "        data_test = data_test.reshape((n_epochs_test * n_split, n_channels, n_samples))\n",
    "        filtered_data = mne.filter.filter_data(data, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False, filter_length=50)\n",
    "        filtered_data_test = mne.filter.filter_data(data_test, sampling_freq, low_cutoff, low_cutoff + 4, verbose = False)\n",
    "        tic = time.time()\n",
    "        train_feats, test_feats = calc_csp(filtered_data, new_train_labels, filtered_data_test, n_components)\n",
    "        toc = time.time()\n",
    "        print(\"Time taken for csp calculations: \", toc-tic)\n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "        \n",
    "        low_cutoff += 2\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9c903e-1265-47cb-a425-fb6cca7719a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_cheby2(train_data, train_labels, test_data, sampling_freq, n_components):\n",
    "\n",
    "    # Why number of bands were set to 24??\n",
    "    [n_epochs_train, n_split, n_channels, n_samples] = train_data.shape\n",
    "    [n_epochs_test, _, _, _] = test_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    low_cutoff = 4\n",
    "    number_of_bands = 16\n",
    "    for b in range(number_of_bands):\n",
    "        data = train_data.copy()\n",
    "        new_train_labels = np.repeat(train_labels, n_split)\n",
    "        data = data.reshape((n_epochs_train * n_split, n_channels, n_samples))\n",
    "        data_test = test_data.copy()\n",
    "        data_test = data_test.reshape((n_epochs_test * n_split, n_channels, n_samples))\n",
    "        \n",
    "        sos1 = signal.cheby2(15, 20, [low_cutoff, low_cutoff + 4], 'bp', fs=250, output='sos')\n",
    "        \n",
    "        filtered_data = signal.sosfilt(sos1, data)\n",
    "        print(\"The filtered data shape:\", filtered_data.shape)\n",
    "        filtered_data_test = signal.sosfilt(sos1, data_test)\n",
    "        tic = time.time()\n",
    "        train_feats, test_feats = calc_csp(filtered_data, new_train_labels, filtered_data_test, n_components)\n",
    "        toc = time.time()\n",
    "        print(\"Time taken for csp calculations: \", toc-tic)\n",
    "        \n",
    "        train_feats, test_feats = np.expand_dims(train_feats, axis=1), np.expand_dims(test_feats, axis=1)\n",
    "        \n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "        \n",
    "        low_cutoff += 2\n",
    "    \n",
    "    train_features = train_features.reshape(n_epochs_train, n_split, number_of_bands, n_components)\n",
    "    test_features  = test_features.reshape(n_epochs_test, n_split, number_of_bands, n_components) \n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0673d03-6d55-4c13-a808-993e8ac9006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess_validator(train_data, train_labels, test_data, sampling_freq, n_components=2):\n",
    "\n",
    "    # Why number of bands were set to 24??\n",
    "    [n_epochs_train, n_split, n_channels, n_samples] = train_data.shape\n",
    "    [n_epochs_test, _, _, _] = test_data.shape\n",
    "    \n",
    "    \n",
    "    \n",
    "    low_cutoff = 4\n",
    "    number_of_bands = 16\n",
    "    for b in range(number_of_bands):\n",
    "        data = train_data.copy()\n",
    "        new_train_labels = np.repeat(train_labels, n_split)\n",
    "        data = data.reshape((n_epochs_train * n_split, n_channels, n_samples))\n",
    "        data_test = test_data.copy()\n",
    "        data_test = data_test.reshape((n_epochs_test * n_split, n_channels, n_samples))\n",
    "        \n",
    "        sos1 = signal.cheby2(40, 20, [low_cutoff, low_cutoff + 4], 'bp', fs=250, output='sos')\n",
    "        \n",
    "        filtered_data = signal.sosfilt(sos1, data)\n",
    "        print(\"The filtered data shape:\", filtered_data.shape)\n",
    "        filtered_data_test = signal.sosfilt(sos1, data_test)\n",
    "        tic = time.time()\n",
    "        train_feats, test_feats = calc_csp(filtered_data, new_train_labels, filtered_data_test, n_components)\n",
    "        toc = time.time()\n",
    "        print(\"Time taken for csp calculations: \", toc-tic)\n",
    "        \n",
    "        train_feats, test_feats = np.expand_dims(train_feats, axis=1), np.expand_dims(test_feats, axis=1)\n",
    "        \n",
    "        if b == 0:\n",
    "            train_features = train_feats\n",
    "            test_features = test_feats\n",
    "        else:\n",
    "            train_features = np.concatenate((train_features, train_feats), axis = 1)\n",
    "            test_features = np.concatenate((test_features, test_feats), axis = 1)\n",
    "        \n",
    "        low_cutoff += 2\n",
    "    \n",
    "    train_features = train_features.reshape(n_epochs_train, n_split, number_of_bands, n_components)\n",
    "    test_features  = test_features.reshape(n_epochs_test, n_split, number_of_bands, n_components) \n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    return train_features, test_features, filtered_data.reshape((n_epochs_train, n_split, n_channels, n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee96c3-4388-4755-a8bb-f3e6894c209b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Device (GPU\\CPU\\MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0fb8a47-c64b-4a77-85b2-d26be87fb7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    # Check if TensorFlow Multi-Process Service (MPS) is available\n",
    "    if tf.config.experimental.list_physical_devices('MPS'):\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81a178-1aea-4351-9b17-009a793872c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# ATCNet Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dfa9c66-b6cf-49c2-aab7-0a03d459bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(model_name, dataset_conf):\n",
    "    \n",
    "    n_classes = dataset_conf.get('n_classes')\n",
    "    n_channels = dataset_conf.get('n_channels')\n",
    "    in_samples = dataset_conf.get('in_samples')\n",
    "\n",
    "    # Select the model\n",
    "    if(model_name == 'ATCNet'):\n",
    "        # Train using the proposed ATCNet model: https://doi.org/10.1109/TII.2022.3197419\n",
    "        model = models.ATCNet_( \n",
    "            # Dataset parameters\n",
    "            n_classes = n_classes, \n",
    "            in_chans = n_channels, \n",
    "            in_samples = in_samples, \n",
    "            # Sliding window (SW) parameter\n",
    "            n_windows = 5, \n",
    "            # Attention (AT) block parameter\n",
    "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
    "            # Convolutional (CV) block parameters\n",
    "            eegn_F1 = 16,\n",
    "            eegn_D = 2, \n",
    "            eegn_kernelSize = 64,\n",
    "            eegn_poolSize = 7,\n",
    "            eegn_dropout = 0.3,\n",
    "            # Temporal convolutional (TC) block parameters\n",
    "            tcn_depth = 2, \n",
    "            tcn_kernelSize = 4,\n",
    "            tcn_filters = 32,\n",
    "            tcn_dropout = 0.3, \n",
    "            tcn_activation='elu'\n",
    "            )     \n",
    "    elif(model_name == 'TCNet_Fusion'):\n",
    "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
    "        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)      \n",
    "    elif(model_name == 'EEGTCNet'):\n",
    "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
    "        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)          \n",
    "    elif(model_name == 'EEGNet'):\n",
    "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
    "        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples) \n",
    "    elif(model_name == 'EEGNeX'):\n",
    "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
    "        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n",
    "    elif(model_name == 'DeepConvNet'):\n",
    "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "    elif(model_name == 'ShallowConvNet'):\n",
    "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
    "        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "    elif(model_name == 'MBEEG_SENet'):\n",
    "        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n",
    "        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd97336-2af3-4cf9-871a-85772e319929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:50:02.885868: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 16:50:05.307952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13625 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:5e:00.0, compute capability: 7.5\n",
      "2024-05-16 16:50:05.310434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13625 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:5f:00.0, compute capability: 7.5\n",
      "2024-05-16 16:50:05.312554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 13625 MB memory:  -> device: 2, name: Tesla T4, pci bus id: 0000:87:00.0, compute capability: 7.5\n",
      "2024-05-16 16:50:05.314720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13625 MB memory:  -> device: 3, name: Tesla T4, pci bus id: 0000:d8:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "dataset = 'BCI2a'\n",
    "in_samples = 1000                   # Changed!!!\n",
    "n_channels = 22\n",
    "n_sub = 9\n",
    "n_classes = 4\n",
    "classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
    "\n",
    "\n",
    "\n",
    "# Set dataset paramters as dataset configuration\n",
    "dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
    "                'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
    "                'isStandard': True, 'LOSO': True}\n",
    "\n",
    "\n",
    "# Set training hyperparamters as train configuration\n",
    "train_conf = { 'batch_size': 64, 'epochs': 1000, 'patience': 300, 'lr': 0.001,\n",
    "              'LearnCurves': True, 'n_train': 10, 'model':'ATCNet'}\n",
    "\n",
    "model = getModel(train_conf.get('model'), dataset_conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69cd7d9d-4247-451f-8175-7d98a228dfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1, 22, 1000  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " permute (Permute)              (None, 1000, 22, 1)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 1000, 22, 16  1024        ['permute[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1000, 22, 16  64         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " depthwise_conv2d (DepthwiseCon  (None, 1000, 1, 32)  704        ['batch_normalization[0][0]']    \n",
      " v2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 1000, 1, 32)  128        ['depthwise_conv2d[0][0]']       \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 1000, 1, 32)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 125, 1, 32)  0           ['activation[0][0]']             \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 125, 1, 32)   0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 125, 1, 32)   16384       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 125, 1, 32)  128         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 125, 1, 32)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 17, 1, 32)   0           ['activation_1[0][0]']           \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 17, 1, 32)    0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 17, 32)       0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 13, 32)      0           ['lambda[0][0]']                 \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1 (Sl  (None, 13, 32)      0           ['lambda[0][0]']                 \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 13, 32)      0           ['lambda[0][0]']                 \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  (None, 13, 32)      0           ['lambda[0][0]']                 \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_4 (Sl  (None, 13, 32)      0           ['lambda[0][0]']                 \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 13, 32)      64          ['tf.__operators__.getitem[0][0]'\n",
      " alization)                                                      ]                                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 13, 32)      64          ['tf.__operators__.getitem_1[0][0\n",
      " rmalization)                                                    ]']                              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 13, 32)      64          ['tf.__operators__.getitem_2[0][0\n",
      " rmalization)                                                    ]']                              \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 13, 32)      64          ['tf.__operators__.getitem_3[0][0\n",
      " rmalization)                                                    ]']                              \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 13, 32)      64          ['tf.__operators__.getitem_4[0][0\n",
      " rmalization)                                                    ]']                              \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 13, 32)      2128        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 13, 32)      2128        ['layer_normalization_1[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 13, 32)      2128        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 13, 32)      2128        ['layer_normalization_3[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 13, 32)      2128        ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 13, 32)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 13, 32)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 13, 32)       0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 13, 32)       0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)           (None, 13, 32)       0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 13, 32)       0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'dropout_2[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 13, 32)       0           ['tf.__operators__.getitem_1[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 13, 32)       0           ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 13, 32)       0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 13, 32)       0           ['tf.__operators__.getitem_4[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'dropout_22[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 13, 32)       4128        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 13, 32)       4128        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 13, 32)       4128        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 13, 32)       4128        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 13, 32)       4128        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 13, 32)      128         ['conv1d[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 13, 32)      128         ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 13, 32)      128         ['conv1d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 13, 32)      128         ['conv1d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 13, 32)      128         ['conv1d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 13, 32)       0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 13, 32)       0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 13, 32)       0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 13, 32)       0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 13, 32)       0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 13, 32)       0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 13, 32)       0           ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 13, 32)       0           ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_18 (Dropout)           (None, 13, 32)       0           ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)           (None, 13, 32)       0           ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 13, 32)       4128        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 13, 32)       4128        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 13, 32)       4128        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 13, 32)       4128        ['dropout_18[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 13, 32)       4128        ['dropout_23[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 13, 32)      128         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 13, 32)      128         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 13, 32)      128         ['conv1d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 13, 32)      128         ['conv1d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 13, 32)      128         ['conv1d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 13, 32)       0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 13, 32)       0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 13, 32)       0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 13, 32)       0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 13, 32)       0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 13, 32)       0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 13, 32)       0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 13, 32)       0           ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 13, 32)       0           ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)           (None, 13, 32)       0           ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 13, 32)       0           ['dropout_4[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 13, 32)       0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 13, 32)       0           ['dropout_14[0][0]',             \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 13, 32)       0           ['dropout_19[0][0]',             \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 13, 32)       0           ['dropout_24[0][0]',             \n",
      "                                                                  'add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 13, 32)       0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 13, 32)       0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 13, 32)       0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 13, 32)       0           ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 13, 32)       0           ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 13, 32)       4128        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 13, 32)       4128        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 13, 32)       4128        ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 13, 32)       4128        ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 13, 32)       4128        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 13, 32)      128         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 13, 32)      128         ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 13, 32)      128         ['conv1d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 13, 32)      128         ['conv1d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 13, 32)      128         ['conv1d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 13, 32)       0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 13, 32)       0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 13, 32)       0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 13, 32)       0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 13, 32)       0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 13, 32)       0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 13, 32)       0           ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 13, 32)       0           ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 13, 32)       0           ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)           (None, 13, 32)       0           ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 13, 32)       4128        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 13, 32)       4128        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 13, 32)       4128        ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 13, 32)       4128        ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 13, 32)       4128        ['dropout_25[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 13, 32)      128         ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 13, 32)      128         ['conv1d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 13, 32)      128         ['conv1d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 13, 32)      128         ['conv1d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 13, 32)      128         ['conv1d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 13, 32)       0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 13, 32)       0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 13, 32)       0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 13, 32)       0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 13, 32)       0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 13, 32)       0           ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 13, 32)       0           ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 13, 32)       0           ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 13, 32)       0           ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)           (None, 13, 32)       0           ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 13, 32)       0           ['dropout_6[0][0]',              \n",
      "                                                                  'activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 13, 32)       0           ['dropout_11[0][0]',             \n",
      "                                                                  'activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 13, 32)       0           ['dropout_16[0][0]',             \n",
      "                                                                  'activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 13, 32)       0           ['dropout_21[0][0]',             \n",
      "                                                                  'activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 13, 32)       0           ['dropout_26[0][0]',             \n",
      "                                                                  'activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 13, 32)       0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 13, 32)       0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 13, 32)       0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 13, 32)       0           ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 13, 32)       0           ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 32)           0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 32)           0           ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 32)           0           ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_4 (Lambda)              (None, 32)           0           ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_5 (Lambda)              (None, 32)           0           ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4)            132         ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            132         ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 4)            132         ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4)            132         ['lambda_4[0][0]']               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 4)            132         ['lambda_5[0][0]']               \n",
      "                                                                                                  \n",
      " average (Average)              (None, 4)            0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]',                \n",
      "                                                                  'dense_2[0][0]',                \n",
      "                                                                  'dense_3[0][0]',                \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " softmax (Activation)           (None, 4)            0           ['average[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 115,172\n",
      "Trainable params: 113,732\n",
      "Non-trainable params: 1,440\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d1b1c25-15aa-46f4-9a46-453b01056e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EEGNet(nb_classes, Chans = 64, Samples = 128, \n",
    "           dropoutRate = 0.5, kernLength = 64, F1 = 8, \n",
    "           D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout',\n",
    "           learning_rate=0.0009):\n",
    "    \"\"\" Keras Implementation of EEGNet\n",
    "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
    "\n",
    "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
    "    version (version v1 and v2 on arxiv). We strongly recommend using this\n",
    "    architecture as it performs much better and has nicer properties than\n",
    "    our earlier version. For example:\n",
    "        \n",
    "        1. Depthwise Convolutions to learn spatial filters within a \n",
    "        temporal convolution. The use of the depth_multiplier option maps \n",
    "        exactly to the number of spatial filters learned within a temporal\n",
    "        filter. This matches the setup of algorithms like FBCSP which learn \n",
    "        spatial filters within each filter in a filter-bank. This also limits \n",
    "        the number of free parameters to fit when compared to a fully-connected\n",
    "        convolution. \n",
    "        \n",
    "        2. Separable Convolutions to learn how to optimally combine spatial\n",
    "        filters across temporal bands. Separable Convolutions are Depthwise\n",
    "        Convolutions followed by (1x1) Pointwise Convolutions. \n",
    "        \n",
    "    \n",
    "    While the original paper used Dropout, we found that SpatialDropout2D \n",
    "    sometimes produced slightly better results for classification of ERP \n",
    "    signals. However, SpatialDropout2D significantly reduced performance \n",
    "    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n",
    "    the default Dropout in most cases.\n",
    "        \n",
    "    Assumes the input signal is sampled at 128Hz. If you want to use this model\n",
    "    for any other sampling rate you will need to modify the lengths of temporal\n",
    "    kernels and average pooling size in blocks 1 and 2 as needed (double the \n",
    "    kernel lengths for double the sampling rate, etc). Note that we haven't \n",
    "    tested the model performance with this rule so this may not work well. \n",
    "    \n",
    "    The model with default parameters gives the EEGNet-8,2 model as discussed\n",
    "    in the paper. This model should do pretty well in general, although it is\n",
    "\tadvised to do some model searching to get optimal performance on your\n",
    "\tparticular dataset.\n",
    "\n",
    "    We set F2 = F1 * D (number of input filters = number of output filters) for\n",
    "    the SeparableConv2D layer. We haven't extensively tested other values of this\n",
    "    parameter (say, F2 < F1 * D for compressed learning, and F2 > F1 * D for\n",
    "    overcomplete). We believe the main parameters to focus on are F1 and D. \n",
    "\n",
    "    Inputs:\n",
    "        \n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.     \n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "    \n",
    "    input1   = Input(shape = (Chans, Samples, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
    "                                   input_shape = (Chans, Samples, 1),\n",
    "                                   use_bias = False)(input1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
    "                                   depth_multiplier = D,\n",
    "                                   depthwise_constraint = max_norm(1.))(block1)\n",
    "    block1       = BatchNormalization()(block1)\n",
    "    block1       = Activation('elu')(block1)\n",
    "    block1       = AveragePooling2D((1, 8))(block1)\n",
    "    block1       = dropoutType(dropoutRate)(block1)\n",
    "    \n",
    "    block2       = SeparableConv2D(F2, (1, 40),\n",
    "                                   use_bias = False, padding = 'same')(block1)\n",
    "    block2       = BatchNormalization()(block2)\n",
    "    block2       = Activation('elu')(block2)\n",
    "    block2       = AveragePooling2D((1, 16))(block2)\n",
    "    block2       = dropoutType(dropoutRate)(block2)\n",
    "        \n",
    "    flatten      = Flatten(name = 'flatten')(block2)\n",
    "    \n",
    "    dense        = Dense(nb_classes, name = 'dense', \n",
    "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
    "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=input1, outputs=softmax)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61c74e-3fdb-4e1a-b086-fc5180a12db4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc620ef-7aad-4b25-bfa7-e03bb8076069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bijan/py3x/Code_Zhang/Transfer_Learning_On_EEG_BCI'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a2cf427-6a5c-47af-ab15-a4d4203b1b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cloud ...\n",
      "Please make sure to modify how you read the data according to your need!\n",
      "\n",
      "\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P16.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P17.set...\n",
      "Not setting metadata\n",
      "160 matching events found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P18.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P19.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P20.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P21.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P22.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P23.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "479 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P24.set...\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P25.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P26.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "479 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P27.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P28.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P29.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n",
      "Extracting parameters from /home/bijan/projects/def-b09sdp/bijan/Phase2/P30.set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23794/959435681.py:29: RuntimeWarning: Unknown types found, setting as type EEG:\n",
      "ref: ['FCz']\n",
      "  raw_data = mne.read_epochs_eeglab(file_path_T)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "480 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    if path[0] == '/home/bijan/py3x/Code_Zhang/Transfer_Learning_On_EEG_BCI':\n",
    "        print(\"Running on cloud ...\")\n",
    "        print(\"Please make sure to modify how you read the data according to your need!\\n\\n\")\n",
    "        raw_data_path = \"/home/bijan/projects/def-b09sdp/bijan/Phase2\"\n",
    "        \n",
    "    elif path[0] == \"'pwd' is not recognized as an internal or external command,\":\n",
    "        print(\"Running local ...\")\n",
    "        print(\"Please make sure to change the data path!\\n\\n\")\n",
    "        raw_data_path = \"D:\\Hadi_BCI\\Recordings\\Phase 2\\PreProcessedData\\P16toP30\"\n",
    "        \n",
    "except NameError:\n",
    "    print(\"Running local ...\")\n",
    "    print(\"Please make sure to change the data path!\\n\\n\")\n",
    "    raw_data_path = \"D:\\Hadi_BCI\\Recordings\\Phase 2\\PreProcessedData\\P16toP30\"\n",
    "    \n",
    "    \n",
    "\n",
    "data_epochs = []\n",
    "data_labels = []\n",
    "all_data_epochs = []\n",
    "all_data_labels = []\n",
    "\n",
    "for participant_id in range(16, 31):\n",
    "    \n",
    "    participant_T = f\"P{participant_id}\"\n",
    "    file_path_T = f\"{raw_data_path}/{participant_T}.set\"\n",
    "\n",
    "    raw_data = mne.read_epochs_eeglab(file_path_T)\n",
    "\n",
    "\n",
    "    epochs, labels = epoch_label_extractor(raw_data, epoch_length=1123, num_channels=64)\n",
    "    \n",
    "    class_1=0 # Left\n",
    "    class_2=1 # Right\n",
    "    class_3=2 # Feet\n",
    "    class_4=5\n",
    "    class_5=6\n",
    "    class_6=7\n",
    "    class_7=8\n",
    "    \n",
    "    classes = [class_1, class_2, class_3, class_4, class_5, class_6, class_7]\n",
    "    num_channels = 64\n",
    "    epoch_length = 1123\n",
    "    \n",
    "    data_epochs, data_labels = class_extraction(classes, epochs, labels)\n",
    "    \n",
    "    all_data_epochs.append(data_epochs)\n",
    "    all_data_labels.append(data_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa62c8e6-7f5c-49eb-999a-6b3ca05ddf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7395c987-0862-41a1-bd42-e9ef75ac3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 train epochs:     (420, 64, 1123)     Subject 1 train labels:     (420, 1)\n",
      "Subject 2 train epochs:     (140, 64, 1123)     Subject 2 train labels:     (140, 1)\n",
      "Subject 3 train epochs:     (420, 64, 1123)     Subject 3 train labels:     (420, 1)\n",
      "Subject 4 train epochs:     (420, 64, 1123)     Subject 4 train labels:     (420, 1)\n",
      "Subject 5 train epochs:     (420, 64, 1123)     Subject 5 train labels:     (420, 1)\n",
      "Subject 6 train epochs:     (420, 64, 1123)     Subject 6 train labels:     (420, 1)\n",
      "Subject 7 train epochs:     (420, 64, 1123)     Subject 7 train labels:     (420, 1)\n",
      "Subject 8 train epochs:     (420, 64, 1123)     Subject 8 train labels:     (420, 1)\n",
      "Subject 9 train epochs:     (140, 64, 1123)     Subject 9 train labels:     (140, 1)\n",
      "Subject 10 train epochs:     (420, 64, 1123)     Subject 10 train labels:     (420, 1)\n",
      "Subject 11 train epochs:     (420, 64, 1123)     Subject 11 train labels:     (420, 1)\n",
      "Subject 12 train epochs:     (420, 64, 1123)     Subject 12 train labels:     (420, 1)\n",
      "Subject 13 train epochs:     (420, 64, 1123)     Subject 13 train labels:     (420, 1)\n",
      "Subject 14 train epochs:     (420, 64, 1123)     Subject 14 train labels:     (420, 1)\n",
      "Subject 15 train epochs:     (420, 64, 1123)     Subject 15 train labels:     (420, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(\"Subject {} train epochs:    \".format(i+1), all_data_epochs[i].shape, \"    Subject {} train labels:    \".format(i+1), all_data_labels[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149bd418-d83e-4bb4-a1d0-29abbaaea54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 5, 6, 6, 5, 2, 8, 7, 8, 5, 1, 1, 8, 0, 8, 8, 1, 0, 2, 1, 2,\n",
       "       0, 6, 7, 7, 7, 0, 2, 8, 7, 5, 8, 0, 5, 0, 5, 7, 6, 5, 8, 8, 7, 8,\n",
       "       7, 2, 1, 2, 2, 1, 2, 1, 1, 1, 0, 2, 8, 6, 1, 5, 7, 6, 7, 0, 7, 6,\n",
       "       5, 5, 7, 5, 0, 2, 6, 2, 5, 1, 6, 2, 2, 2, 0, 0, 5, 0, 6, 5, 0, 8,\n",
       "       0, 5, 7, 2, 1, 7, 6, 1, 8, 5, 1, 6, 2, 0, 6, 7, 5, 0, 1, 1, 7, 0,\n",
       "       6, 6, 8, 8, 2, 8, 7, 5, 6, 1, 8, 6, 6, 2, 1, 6, 2, 0, 2, 5, 8, 7,\n",
       "       1, 1, 8, 6, 0, 8, 5, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_labels[1][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ea1f48a-9b7a-4bf0-a8de-bfd383fd502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Number of events</th>\n",
       "        <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Events</th>\n",
       "        \n",
       "        <td>Feet: 60<br/>Fs: 60<br/>Left: 60<br/>Ls: 60<br/>Rest: 60<br/>Right: 60<br/>Rs: 60<br/>Si: 60</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Time range</th>\n",
       "        <td>-0.496 – 3.992 s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Baseline</th>\n",
       "        <td>off</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<EpochsEEGLAB |  480 events (all good), -0.496 – 3.992 s, baseline off, ~263.3 MB, data loaded,\n",
       " 'Rest': 60\n",
       " 'Right': 60\n",
       " 'Left': 60\n",
       " 'Feet': 60\n",
       " 'Si': 60\n",
       " 'Rs': 60\n",
       " 'Ls': 60\n",
       " 'Fs': 60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3b44f23-4e9a-4b10-8a58-a3a5d6ce40cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data epoch shape (sub 0):     (420, 1, 64, 1123)     All label shape (sub 0):      (420, 1)\n",
      "All data epoch shape (sub 1):     (140, 1, 64, 1123)     All label shape (sub 1):      (140, 1)\n",
      "All data epoch shape (sub 2):     (420, 1, 64, 1123)     All label shape (sub 2):      (420, 1)\n",
      "All data epoch shape (sub 3):     (420, 1, 64, 1123)     All label shape (sub 3):      (420, 1)\n",
      "All data epoch shape (sub 4):     (420, 1, 64, 1123)     All label shape (sub 4):      (420, 1)\n",
      "All data epoch shape (sub 5):     (420, 1, 64, 1123)     All label shape (sub 5):      (420, 1)\n",
      "All data epoch shape (sub 6):     (420, 1, 64, 1123)     All label shape (sub 6):      (420, 1)\n",
      "All data epoch shape (sub 7):     (420, 1, 64, 1123)     All label shape (sub 7):      (420, 1)\n",
      "All data epoch shape (sub 8):     (140, 1, 64, 1123)     All label shape (sub 8):      (140, 1)\n",
      "All data epoch shape (sub 9):     (420, 1, 64, 1123)     All label shape (sub 9):      (420, 1)\n",
      "All data epoch shape (sub 10):     (420, 1, 64, 1123)     All label shape (sub 10):      (420, 1)\n",
      "All data epoch shape (sub 11):     (420, 1, 64, 1123)     All label shape (sub 11):      (420, 1)\n",
      "All data epoch shape (sub 12):     (420, 1, 64, 1123)     All label shape (sub 12):      (420, 1)\n",
      "All data epoch shape (sub 13):     (420, 1, 64, 1123)     All label shape (sub 13):      (420, 1)\n",
      "All data epoch shape (sub 14):     (420, 1, 64, 1123)     All label shape (sub 14):      (420, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_data_epochs)):\n",
    "    all_data_epochs[i] = np.expand_dims(all_data_epochs[i], 1)\n",
    "    print(\"All data epoch shape (sub {}):    \".format(i), all_data_epochs[i].shape, \"    All label shape (sub {}):     \".format(i), all_data_labels[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ad8cc53-ff96-4ce2-8f8e-54f46ec896bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [7],\n",
       "       [5],\n",
       "       [6],\n",
       "       [6],\n",
       "       [5],\n",
       "       [2],\n",
       "       [8],\n",
       "       [7],\n",
       "       [8]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_labels[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d77c53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "433703fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_encode = copy.deepcopy(all_data_labels)\n",
    "encoded = encoder(all_data_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26a1f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs Length: 15\n",
      "labels Length: 15\n",
      "\n",
      "\n",
      "\n",
      "Participant 16 - Epochs[0] shape: (420, 1)\n",
      "Participant 16 - labels[0] shape: (420, 7)\n",
      "\n",
      "\n",
      "\n",
      "Participant 16 - labels[0]:\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Epochs Length:\", len(all_data_labels))\n",
    "print(\"labels Length:\", len(encoded))\n",
    "print('\\n\\n')\n",
    "print(\"Participant 16 - Epochs[0] shape:\", no_encode[3].shape)\n",
    "print(\"Participant 16 - labels[0] shape:\", encoded[3].shape)\n",
    "print('\\n\\n')\n",
    "print(\"Participant 16 - labels[0]:\")\n",
    "print(all_data_labels[3][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbccfbb-49ff-461f-b50f-0f7448e933ab",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Within Subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d55a8f-6ef7-4d36-b1a9-cfb8c89122a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = len(all_data_epochs)\n",
    "all_tests_true = []\n",
    "all_tests_pred = []\n",
    "max_epochs = 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(num_subjects):\n",
    "    \n",
    "    epochs_subject = all_data_epochs[j]\n",
    "    labels_subject = encoded[j]\n",
    "    labels_subject_no_encode = no_encode[j]\n",
    "    \n",
    "    print(labels_subject_no_encode.shape)\n",
    "    \n",
    "    kf_outer2 = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "    \n",
    "    tests_pred = []\n",
    "    tests_true = []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf_outer2.split(epochs_subject, labels_subject_no_encode[:, 0])):\n",
    "\n",
    "\n",
    "\n",
    "        train_epochs = np.array([epochs_subject[j] for j in train_index])\n",
    "        test_epochs = np.array([epochs_subject[k] for k in test_index])\n",
    "        train_labels = np.array([labels_subject[l] for l in train_index])\n",
    "        test_labels = np.array([labels_subject[m] for m in test_index])\n",
    "        no_encoded_train_labels = np.array([labels_subject_no_encode[n] for n in train_index])\n",
    "        no_encoded_test_labels = np.array([labels_subject_no_encode[o] for o in test_index])\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"Outer Loop {}\".format(i+1), \"\\n\")\n",
    "        print(\"      Train epochs' shape:                               \", train_epochs.shape)\n",
    "        print(\"      Test epochs' shape:                                \", test_epochs.shape)\n",
    "        print(\"      Train labels' shape:                               \", train_labels.shape)\n",
    "        print(\"      Test labels' shape:                                \", test_labels.shape)\n",
    "        print(\"      Train labels' shape (without encoding):            \", no_encoded_train_labels.shape)\n",
    "\n",
    "        print(\"      Test labels' shape (without encoding):             \", no_encoded_test_labels.shape)\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "\n",
    "        dataset = 'BCI2a'\n",
    "        in_samples = 1123                   # Changed!!!\n",
    "        n_channels = 64\n",
    "        n_sub = 9\n",
    "        n_classes = 7\n",
    "\n",
    "\n",
    "        # Set dataset paramters as dataset configuration\n",
    "        dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
    "                        'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
    "                        'isStandard': True, 'LOSO': True}\n",
    "\n",
    "\n",
    "        # Set training hyperparamters as train configuration\n",
    "        train_conf = { 'batch_size': 64, 'epochs': 1000, 'patience': 300, 'lr': 0.001,\n",
    "                      'LearnCurves': True, 'n_train': 10, 'model':'ATCNet'}\n",
    "\n",
    "        model = getModel(train_conf.get('model'), dataset_conf)\n",
    "\n",
    "\n",
    "        model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=train_conf.get('lr')), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        patience = 30\n",
    "        train_loss_epochs = []\n",
    "        train_acc_epochs = []\n",
    "        test_loss_epochs = []\n",
    "        test_acc_epochs = []\n",
    "        train_conf_mat = []\n",
    "\n",
    "        # Define early stopping\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        \n",
    "        \n",
    "        # Normalizing the features\n",
    "        mean = train_epochs.mean(axis=(0, 3), keepdims=True)\n",
    "        std = train_epochs.std(axis=(0, 3), keepdims=True)\n",
    "\n",
    "        print(mean.shape)\n",
    "        print(std.shape)\n",
    "\n",
    "        norm_train_epochs = (train_epochs - mean) / std\n",
    "        norm_test_epochs = (test_epochs - mean) / std\n",
    "        \n",
    "        \n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            norm_train_epochs, train_labels,\n",
    "            validation_data=(norm_test_epochs, test_labels),\n",
    "            epochs=max_epochs,\n",
    "            batch_size=32,  # Adjust as needed\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        y_pred_test = model.predict(norm_test_epochs)\n",
    "        y_true_test = test_labels\n",
    "\n",
    "        \n",
    "\n",
    "        # Collect training metrics\n",
    "        train_loss_epochs.extend(history.history['loss'])\n",
    "        train_acc_epochs.extend(history.history['accuracy'])\n",
    "\n",
    "        # Collect testing metrics\n",
    "        test_loss_epochs.extend(history.history['val_loss'])\n",
    "        test_acc_epochs.extend(history.history['val_accuracy'])\n",
    "\n",
    "        epochs_range = np.arange(1, len(train_loss_epochs[:-1*patience])+1)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.suptitle(\"Participant {} - Fold {} - Within Subject\".format(j+1, i+1))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(epochs_range, train_loss_epochs[:-1*patience])\n",
    "        plt.plot(epochs_range, test_loss_epochs[:-1*patience])\n",
    "        plt.legend([\"Train\", \"Test\"])\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(epochs_range, train_acc_epochs[:-1*patience])\n",
    "        plt.plot(epochs_range, test_acc_epochs[:-1*patience])\n",
    "        plt.legend([\"Train\", \"Test\"])\n",
    "        plt.savefig(\"P{}_F{}_WS.jpg\".format(j+1, i+1))\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "        tests_pred.extend(y_pred_test)\n",
    "        tests_true.extend(y_true_test)\n",
    "    \n",
    "    with open(\"y_pred_ParID_{}_ws.pickle\".format(j+1), 'wb') as f:\n",
    "        pickle.dump(tests_pred, f)\n",
    "\n",
    "    with open(\"y_true_ParID_{}_ws.pickle\".format(j+1), 'wb') as f:\n",
    "        pickle.dump(tests_true, f)\n",
    "        \n",
    "    all_tests_pred.append(tests_pred)\n",
    "    all_tests_true.append(tests_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b7317-2f2b-427d-a0c5-4b2ad7719877",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3291b7-f6d9-45fb-a772-9660b71b43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_tests_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ec3fc-0aa8-4f87-b5ab-30fc25f1369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.concatenate(all_tests_pred[4]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4ea157-3a96-4d27-ba8f-9605c2c75bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = []\n",
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "\n",
    "for i in range(1, 15):\n",
    "    \n",
    "    with open(\"y_pred_ParID_{}_ws.pickle\".format(i), 'rb') as f:\n",
    "        all_y_pred.append(pickle.load(f))\n",
    "        \n",
    "        \n",
    "    with open(\"y_true_ParID_{}_ws.pickle\".format(i), 'rb') as f:\n",
    "        all_y_true.append(pickle.load(f))\n",
    "    \n",
    "    y_true = np.array(all_y_true[i-1])\n",
    "    y_pred = np.array(all_y_pred[i-1])\n",
    "    y_true = y_true.argmax(axis=1)\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    confusion_matrices.append(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6aa967db-03b8-4e09-8091-0d92eeb35eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(confusion_matrices)):\n",
    "    print(len(confusion_matrices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ebdd581-553d-4aed-8587-6f272aae0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = pd.DataFrame(sum(confusion_matrices), index=['Left (True)', 'Right (True)', 'Feet (True)', 'SI (True)', 'LS (True)', 'RS (True)', 'FS (True)'], columns=['Left (Pred)', 'Right (Pred)', 'Feet (Pred)', 'SI (Pred)', 'LS (Pred)', 'RS (Pred)', 'FS (Pred)'])\n",
    "normalized = summation.div(summation.sum(axis=1), axis=0)\n",
    "acc = normalized.values.trace() / 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c4c5f0d-9b8b-4556-8846-94475c43677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Hadi - Phase II (7-Class)\n",
      "The within-subject scenario:\n",
      "Algorithm: ATCNet\n",
      "Accuracy:  0.4825187969924812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left (Pred)</th>\n",
       "      <th>Right (Pred)</th>\n",
       "      <th>Feet (Pred)</th>\n",
       "      <th>SI (Pred)</th>\n",
       "      <th>LS (Pred)</th>\n",
       "      <th>RS (Pred)</th>\n",
       "      <th>FS (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Left (True)</th>\n",
       "      <td>0.418421</td>\n",
       "      <td>0.109211</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.069737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right (True)</th>\n",
       "      <td>0.073684</td>\n",
       "      <td>0.459211</td>\n",
       "      <td>0.048684</td>\n",
       "      <td>0.057895</td>\n",
       "      <td>0.035526</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>0.271053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feet (True)</th>\n",
       "      <td>0.085526</td>\n",
       "      <td>0.071053</td>\n",
       "      <td>0.482895</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>0.102632</td>\n",
       "      <td>0.090789</td>\n",
       "      <td>0.051316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI (True)</th>\n",
       "      <td>0.043421</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.134211</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.072368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS (True)</th>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.048684</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.130263</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.047368</td>\n",
       "      <td>0.039474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS (True)</th>\n",
       "      <td>0.240789</td>\n",
       "      <td>0.073684</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.059211</td>\n",
       "      <td>0.061842</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.078947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FS (True)</th>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.244737</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.057895</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.098684</td>\n",
       "      <td>0.464474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Left (Pred)  Right (Pred)  Feet (Pred)  SI (Pred)  LS (Pred)  \\\n",
       "Left (True)      0.418421      0.109211     0.071053   0.042105   0.065789   \n",
       "Right (True)     0.073684      0.459211     0.048684   0.057895   0.035526   \n",
       "Feet (True)      0.085526      0.071053     0.482895   0.115789   0.102632   \n",
       "SI (True)        0.043421      0.050000     0.134211   0.500000   0.136842   \n",
       "LS (True)        0.051316      0.048684     0.064474   0.130263   0.618421   \n",
       "RS (True)        0.240789      0.073684     0.051316   0.059211   0.061842   \n",
       "FS (True)        0.040789      0.244737     0.051316   0.057895   0.042105   \n",
       "\n",
       "              RS (Pred)  FS (Pred)  \n",
       "Left (True)    0.223684   0.069737  \n",
       "Right (True)   0.053947   0.271053  \n",
       "Feet (True)    0.090789   0.051316  \n",
       "SI (True)      0.063158   0.072368  \n",
       "LS (True)      0.047368   0.039474  \n",
       "RS (True)      0.434211   0.078947  \n",
       "FS (True)      0.098684   0.464474  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset: Hadi - Phase II (7-Class)\")\n",
    "print(\"The within-subject scenario:\")\n",
    "print(\"Algorithm: ATCNet\")\n",
    "print(\"Accuracy: \", acc)\n",
    "\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bd396-0100-47f1-87bf-51160149d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da8839-f7ca-4302-b2cd-d39aecd2b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(summation / 576 * np.eye(4, 4)).sum() / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d5222-55aa-4e69-aa7c-59f9a001736d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Cross-subjects (With hyperparameter tuning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c468c-db9a-4d3f-8138-d4af320bfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "class_numbers=4\n",
    "num_subjects = len(all_data_epochs)\n",
    "all_tests_true = []\n",
    "all_tests_pred = []\n",
    "max_epochs = 500\n",
    "learning_rate=0.0009\n",
    "patience = 50\n",
    "\n",
    "\n",
    "kf_outer2 = KFold(n_splits=num_subjects, shuffle=True, random_state=2)    # Split the data into Train_CrossVal and test sets.\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf_outer2.split(all_data_epochs)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_epochs = np.concatenate([all_data_epochs[j] for j in train_index])\n",
    "    test_epochs = np.concatenate([all_data_epochs[k] for k in test_index])\n",
    "    train_labels = np.concatenate([encoded[l] for l in train_index])\n",
    "    test_labels = np.concatenate([encoded[m] for m in test_index])\n",
    "    no_encoded_train_labels = np.concatenate([no_encode[n] for n in train_index])\n",
    "    no_encoded_test_labels = np.concatenate([no_encode[o] for o in test_index])\n",
    "    train_ids_for_save = [participants[i] for i in train_index]\n",
    "    test_ids_for_save = [participants[i] for i in test_index]\n",
    "    \n",
    "    \n",
    "    print(\"Outer Loop {}\".format(i+1), \"\\n\")\n",
    "    print(\"      Train epochs' shape:                               \", train_epochs.shape)\n",
    "\n",
    "    print(\"      Test epochs' shape:                                \", test_epochs.shape)\n",
    "    print(\"      Test labels' shape:                                \", test_labels.shape)\n",
    "    print(\"      Train labels' shape (without encoding):            \", no_encoded_train_labels.shape)\n",
    "\n",
    "    print(\"      Test labels' shape (without encoding):             \", no_encoded_test_labels.shape)\n",
    "    print(\"      Train index:                                       \", train_ids_for_save)\n",
    "\n",
    "    print(\"      Test index:                                        \", test_ids_for_save)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    # Define early stopping\n",
    "    # early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    # All_models = KerasClassifier(model=EEGNet_Modified, epochs=1, \n",
    "    #                              nb_classes=4, Chans=64, Samples=1123, dropoutRate=0.4, \n",
    "    #                              kernLength=64, F1=8, D=4, F2=32, norm_rate=0.25, dropoutType=Dropout,\n",
    "    #                              learning_rate=0.0009)\n",
    "\n",
    "    \n",
    "    \n",
    "    train_loss_epochs = []\n",
    "    train_acc_epochs = []\n",
    "    test_loss_epochs = []\n",
    "    test_acc_epochs = []\n",
    "    train_conf_mat = []\n",
    "    \n",
    "    # # Define hyperparameters and values to search\n",
    "    # param_grid = {\n",
    "    #     'nb_classes': [4],\n",
    "    #     'Chans': [64],\n",
    "    #     'Samples': [1123],\n",
    "    #     'dropoutRate': [0.2, 0.4, 0.5],\n",
    "    #     'kernLength': [32, 64, 125, 150, 300, 500],\n",
    "    #     'F1': [8],\n",
    "    #     'D': [4],\n",
    "    #     'F2': [32],\n",
    "    #     'norm_rate': [0.25],\n",
    "    #     'dropoutType': ['Dropout'],\n",
    "    #     'learning_rate': [0.005],\n",
    "    # }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = 'BCI2a'\n",
    "    in_samples = 1123                   # Changed!!!\n",
    "    n_channels = 64\n",
    "    n_classes = 7\n",
    "    \n",
    "    \n",
    "    # Set dataset paramters as dataset configuration\n",
    "    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
    "                    'n_channels': n_channels, 'in_samples': in_samples, 'isStandard': True, 'LOSO': True}\n",
    "\n",
    "\n",
    "    # Set training hyperparamters as train configuration\n",
    "    train_conf = { 'batch_size': 64, 'epochs': 1123, 'patience': 50, 'lr': 0.001,\n",
    "                  'LearnCurves': True, 'n_train': 10, 'model':'ATCNet'}\n",
    "\n",
    "    model = getModel(train_conf.get('model'), dataset_conf)\n",
    "    \n",
    "\n",
    "    model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=train_conf.get('lr')), metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # Normalizing the features\n",
    "    mean = train_epochs.mean(axis=(0, 3), keepdims=True)\n",
    "    std = train_epochs.std(axis=(0, 3), keepdims=True)\n",
    "\n",
    "    print(mean.shape)\n",
    "    print(std.shape)\n",
    "\n",
    "    norm_train_epochs = (train_epochs - mean) / std\n",
    "    norm_test_epochs = (test_epochs - mean) / std\n",
    "        \n",
    "    \n",
    "    \n",
    "    history = model.fit(\n",
    "        norm_train_epochs, train_labels,\n",
    "        validation_data=(norm_test_epochs, test_labels),\n",
    "        epochs=max_epochs,\n",
    "        batch_size=32,  # Adjust as needed\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    y_pred_test = model.predict(norm_test_epochs)\n",
    "    y_true_test = test_labels\n",
    "    \n",
    "    \n",
    "    with open(\"y_pred_testID_{}.pickle\".format(participants[test_index[0]]), 'wb') as f:\n",
    "        pickle.dump(y_pred_test, f)\n",
    "    \n",
    "    with open(\"y_true_testID_{}.pickle\".format(participants[test_index[0]]), 'wb') as f:\n",
    "        pickle.dump(y_true_test, f)\n",
    "    \n",
    "    # Collect training metrics\n",
    "    train_loss_epochs.extend(history.history['loss'])\n",
    "    train_acc_epochs.extend(history.history['accuracy'])\n",
    "\n",
    "    # Collect testing metrics\n",
    "    test_loss_epochs.extend(history.history['val_loss'])\n",
    "    test_acc_epochs.extend(history.history['val_accuracy'])\n",
    "    \n",
    "    epochs_range = np.arange(1, len(train_loss_epochs[:-1*patience])+1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.suptitle(\"Participant {}\".format(participants[test_index[0]]))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.plot(epochs_range, train_loss_epochs[:-1*patience])\n",
    "    plt.plot(epochs_range, test_loss_epochs[:-1*patience])\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.plot(epochs_range, train_acc_epochs[:-1*patience])\n",
    "    plt.plot(epochs_range, test_acc_epochs[:-1*patience])\n",
    "    plt.legend([\"Train\", \"Test\"])\n",
    "    plt.savefig(\"P{}.jpg\".format(participants[test_index[0]]))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24428f-8c08-4205-8517-0293b9175653",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe426d-41ad-481a-970a-1394c9759973",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee635c-88ba-4f2d-8593-3b366b0dfaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_models.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90711878-f645-443a-9687-07148c1c7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(All_models.get_params().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca1ee2d3-427e-40f7-9212-3eb01de03382",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = []\n",
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "\n",
    "for i in range(1, 16):\n",
    "    with open(\"y_pred_testID_{}.pickle\".format(i), 'rb') as f:\n",
    "        all_y_pred.append(pickle.load(f))\n",
    "        \n",
    "        \n",
    "    with open(\"y_true_testID_{}.pickle\".format(i), 'rb') as f:\n",
    "        all_y_true.append(pickle.load(f))\n",
    "        \n",
    "    y_true = all_y_true[i-2]\n",
    "    y_pred = all_y_pred[i-2]\n",
    "    y_true = y_true.argmax(axis=1)\n",
    "    y_pred = y_pred.argmax(axis=1)\n",
    "    confusion_matrices.append(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbedbd-ff0c-4b8b-9cf8-bc51c0c2418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confusion_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb4caf-83ee-4853-a6ab-643e52d5af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices[7] = np.pad(confusion_matrices[7], (0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fd5796f-1cf4-4f51-9ab3-f2735b410bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = pd.DataFrame(sum(confusion_matrices), index=['Left (True)', 'Right (True)', 'Feet (True)', 'SI (True)', 'LS (True)', 'RS (True)', 'FS (True)'], columns=['Left (Pred)', 'Right (Pred)', 'Feet (Pred)', 'SI (Pred)', 'LS (Pred)', 'RS (Pred)', 'FS (Pred)'])\n",
    "normalized = summation.div(summation.sum(axis=1), axis=0)\n",
    "acc = normalized.values.trace() / 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234527d-ee6f-4084-bd58-09bbffc2e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a26125-1e6f-4ab0-b029-cc6075356f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e38d298-983b-4399-b59d-fbfb65986317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Hadi - Phase 2 (7-Class)\n",
      "The cross-subject scenario:\n",
      "Algorithm: ATCNet\n",
      "Accuracy:  0.30261324041811843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left (Pred)</th>\n",
       "      <th>Right (Pred)</th>\n",
       "      <th>Feet (Pred)</th>\n",
       "      <th>SI (Pred)</th>\n",
       "      <th>LS (Pred)</th>\n",
       "      <th>RS (Pred)</th>\n",
       "      <th>FS (Pred)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Left (True)</th>\n",
       "      <td>0.337805</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.034146</td>\n",
       "      <td>0.068293</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>0.282927</td>\n",
       "      <td>0.093902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Right (True)</th>\n",
       "      <td>0.074390</td>\n",
       "      <td>0.381707</td>\n",
       "      <td>0.032927</td>\n",
       "      <td>0.064634</td>\n",
       "      <td>0.046341</td>\n",
       "      <td>0.043902</td>\n",
       "      <td>0.356098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feet (True)</th>\n",
       "      <td>0.131707</td>\n",
       "      <td>0.089024</td>\n",
       "      <td>0.104878</td>\n",
       "      <td>0.276829</td>\n",
       "      <td>0.229268</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>0.081707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SI (True)</th>\n",
       "      <td>0.078049</td>\n",
       "      <td>0.095122</td>\n",
       "      <td>0.119512</td>\n",
       "      <td>0.302439</td>\n",
       "      <td>0.241463</td>\n",
       "      <td>0.065854</td>\n",
       "      <td>0.097561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LS (True)</th>\n",
       "      <td>0.081707</td>\n",
       "      <td>0.120732</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.259756</td>\n",
       "      <td>0.393902</td>\n",
       "      <td>0.062195</td>\n",
       "      <td>0.030488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RS (True)</th>\n",
       "      <td>0.352439</td>\n",
       "      <td>0.110976</td>\n",
       "      <td>0.041463</td>\n",
       "      <td>0.065854</td>\n",
       "      <td>0.080488</td>\n",
       "      <td>0.275610</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FS (True)</th>\n",
       "      <td>0.075610</td>\n",
       "      <td>0.342683</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.104878</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.321951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Left (Pred)  Right (Pred)  Feet (Pred)  SI (Pred)  LS (Pred)  \\\n",
       "Left (True)      0.337805      0.100000     0.034146   0.068293   0.082927   \n",
       "Right (True)     0.074390      0.381707     0.032927   0.064634   0.046341   \n",
       "Feet (True)      0.131707      0.089024     0.104878   0.276829   0.229268   \n",
       "SI (True)        0.078049      0.095122     0.119512   0.302439   0.241463   \n",
       "LS (True)        0.081707      0.120732     0.051220   0.259756   0.393902   \n",
       "RS (True)        0.352439      0.110976     0.041463   0.065854   0.080488   \n",
       "FS (True)        0.075610      0.342683     0.051220   0.104878   0.048780   \n",
       "\n",
       "              RS (Pred)  FS (Pred)  \n",
       "Left (True)    0.282927   0.093902  \n",
       "Right (True)   0.043902   0.356098  \n",
       "Feet (True)    0.086585   0.081707  \n",
       "SI (True)      0.065854   0.097561  \n",
       "LS (True)      0.062195   0.030488  \n",
       "RS (True)      0.275610   0.073171  \n",
       "FS (True)      0.054878   0.321951  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset: Hadi - Phase 2 (7-Class)\")\n",
    "print(\"The cross-subject scenario:\")\n",
    "print(\"Algorithm: ATCNet\")\n",
    "print(\"Accuracy: \", acc)\n",
    "\n",
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fa6dc-4794-4e57-b493-80ce26effec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([1, 1, 2, 2, 3, 3, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba87097-1690-404c-b051-55f9c13c3473",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test for the effect of calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11fff1-54d3-4dcc-8ac1-bb29e3b1e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "Calibrated_model = []\n",
    "for file in os.listdir(\"/home/bijan/py3x\"):\n",
    "    if file.endswith(\".h5\") and file.startswith(\"Calibrated\"):\n",
    "        Calibrated_model.append(file)\n",
    "    elif file.endswith(\".h5\") and file.startswith(\"Model\"):\n",
    "        models.append(file)\n",
    "        \n",
    "Calibrated_model = sorted(Calibrated_model)\n",
    "models = sorted(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6d2b9-976c-48c7-8f69-ca5033ad00dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50466690-38ae-4f90-9012-1457989185c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "Calibrated_models = {}\n",
    "for i in range(14):\n",
    "    if i == 8:\n",
    "        continue\n",
    "        \n",
    "    for j in range(6):\n",
    "        #print(\"Model{}{}.h5\".format(i+1, j+1))\n",
    "        model_name = \"Model{}{}.h5\".format(i+1, j+1)\n",
    "        Calibrated_model_name = \"Calibrated_Model{}{}.h5\".format(i+1, j+1)\n",
    "        models[\"{:02}{:02}\".format(i+1, j+1)] = load_model(model_name)\n",
    "        Calibrated_models[\"{:02}{:02}\".format(i+1, j+1)] = load_model(Calibrated_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10d86e-1611-4280-8da5-a61bcf95bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{:02}{:02}\".format(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5a0ea-0c50-4ec8-873a-327b68baee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"1406\"].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34579d-f2e3-4917-9757-ef6058078bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Calibrated_models[\"1406\"].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33991647-0cda-498a-9e52-6950779cc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = len(EEG_epochs)\n",
    "all_tests_true = []\n",
    "all_tests_pred = []\n",
    "models = {}\n",
    "Calibrated_models = {}\n",
    "\n",
    "kf_outer1 = KFold(n_splits=6, shuffle=True, random_state=42)              # Split the data into Train and Cross-Validation sets\n",
    "kf_outer2 = KFold(n_splits=num_subjects, shuffle=True, random_state=2)    # Split the data into Train_CrossVal and test sets.\n",
    "\n",
    "\n",
    "for i, (train_crossval_index, test_index) in enumerate(kf_outer2.split(EEG_epochs)):\n",
    "    \n",
    "    if test_index == 7:\n",
    "        continue\n",
    "    \n",
    "    train_crossval = [EEG_epochs[i] for i in train_crossval_index]\n",
    "    test_epochs = np.concatenate([EEG_epochs[i] for i in test_index])\n",
    "    train_crossval_labels = [encoded[i] for i in train_crossval_index]\n",
    "    test_labels = np.concatenate([encoded[i] for i in test_index])\n",
    "    no_encoded_train_crossval = [no_encode[i] for i in train_crossval_index]\n",
    "    no_encoded_test = np.concatenate([no_encode[i] for i in test_index])\n",
    "\n",
    "    temp_pred = []\n",
    "    temp_true = []\n",
    "\n",
    "    for j, (train_index, val_index) in enumerate(kf_outer1.split(train_crossval)):\n",
    "        \n",
    "        \n",
    "        train_epochs = np.concatenate([train_crossval[i] for i in train_index])\n",
    "        crossval_epochs = np.concatenate([train_crossval[i] for i in val_index])\n",
    "        train_labels = np.concatenate([train_crossval_labels[i] for i in train_index])\n",
    "        crossval_labels = np.concatenate([train_crossval_labels[i] for i in val_index])\n",
    "        no_encoded_train = np.concatenate([no_encoded_train_crossval[i] for i in train_index])\n",
    "        no_encoded_crossval = np.concatenate([no_encoded_train_crossval[i] for i in val_index])\n",
    "        train_ids_for_save = [train_crossval_index[i] for i in train_index]\n",
    "        cross_val_ids_for_save = [train_crossval_index[i] for i in val_index]\n",
    "        \n",
    "        \n",
    "        print(\"Outer Loop {} and Inner Loop {}:\".format(i+1, j+1), \"\\n\")\n",
    "        print(\"      Train epochs' shape:                               \", train_epochs.shape)\n",
    "        #print(\"     Train labels' shape:                               \", train_labels.shape)\n",
    "        print(\"      Cross-validation epochs' shape:                    \", crossval_epochs.shape)\n",
    "        #print(\"     Cross-validation labels' shape:                    \", crossval_labels.shape)\n",
    "        print(\"      Test epochs' shape:                                \", test_epochs.shape)\n",
    "        #print(\"     Test labels' shape:                                \", test_labels.shape)\n",
    "        #print(\"     Train labels' shape (without encoding):            \", no_encoded_train.shape)\n",
    "        #print(\"     Cross-validation labels' shape (without encoding): \", no_encoded_crossval.shape)\n",
    "        #print(\"     Test labels' shape (without encoding):             \", no_encoded_test.shape)\n",
    "        print(\"      Train index:                                      \", train_ids_for_save)\n",
    "        print(\"      Cross-validation index:                           \", cross_val_ids_for_save)\n",
    "        print(\"      Test index:                                       \", test_index)\n",
    "        print('\\n\\n')\n",
    "        \n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = open('temp_stdout{}.txt'.format(i), 'w')  # Redirect output to a temporary file\n",
    "        train_features, CrossVal_features, test_features = feature_extraction_cv(train_epochs, no_encoded_train, crossval_epochs, test_epochs, number_of_bands=9, sampling_freq=250, low_cutoff=0, number_of_components=64)\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = original_stdout\n",
    "    \n",
    "        print(\"Train features shape:            \", train_features.shape)\n",
    "        print(\"Cross-validation features shape: \", CrossVal_features.shape)\n",
    "        print(\"Test features shape:             \", test_features.shape)\n",
    "    \n",
    "        model_name = \"Model{}{}.h5\".format(i+1, j+1)\n",
    "        Calibrated_model_name = \"Calibrated_Model{}{}.h5\".format(i+1, j+1)\n",
    "        models[\"{:02}{:02}\".format(i+1, j+1)] = load_model(model_name)\n",
    "        Calibrated_models[\"{:02}{:02}\".format(i+1, j+1)] = load_model(Calibrated_model_name)\n",
    "        \n",
    "        temp_pred.append(models[\"{:02}{:02}\".format(i+1, j+1)].predict(test_features[60:]))\n",
    "        temp_true.append(test_labels[60:])\n",
    "        \n",
    "    all_tests_pred.append(temp_pred)\n",
    "    all_tests_true.append(temp_true)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b1db8-fd82-4248-af7d-378f87f1f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"all_tests_pred_without_calibration\", \"wb\") as fp:\n",
    "    pickle.dump(all_tests_pred, fp)\n",
    "\n",
    "with open(\"all_tests_true_without_calibration\", \"wb\") as fp:\n",
    "    pickle.dump(all_tests_true, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d30ebb-08d2-48c0-8cfd-dc3915c5da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices_ap = []\n",
    "for i in range(len(all_tests_pred)):\n",
    "    for j in range(len(all_tests_pred[1])):\n",
    "        y_true = 2 - np.argmax(all_tests_true[i][j], axis=1)\n",
    "        y_pred = 2 - np.argmax(all_tests_pred[i][j], axis=1)\n",
    "    confusion_matrices_ap.append(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd2c7b-5dbb-4b67-b72a-15e759258c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = pd.DataFrame(sum(confusion_matrices_ap), index=['class 1 (True)', 'class 2 (True)'], columns=['class 1 (Pred)', 'class 2 (Pred)'])\n",
    "summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792408a4-5e58-4ca5-a1c5-6e73b9dcd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "summation / 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69570fb4-4782-4b2d-a397-a3f1ebac8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_confusion_matrix = sum(confusion_matrices_ap) / len(confusion_matrices_ap)\n",
    "\n",
    "true_positive = mean_confusion_matrix[1, 1]\n",
    "true_negative = mean_confusion_matrix[0, 0]\n",
    "false_positive = mean_confusion_matrix[0, 1]\n",
    "false_negative = mean_confusion_matrix[1, 0]\n",
    " \n",
    "accuracy = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "sensitivity = recall  # Same as recall\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Average metrics for binary classification (Left vs Right hand):\\n\\n\")\n",
    "print(f\"       Accuracy:                  {accuracy:.2f}\\n\")\n",
    "print(f\"       Precision:                 {precision:.2f}\\n\")\n",
    "print(f\"       Recall (Sensitivity):      {recall:.2f}     \\n\")\n",
    "print(f\"       F1 Score:                  {f1:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14849d89-2fc8-4a98-9e93-a10cced14d49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test of the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f844f66-90de-4f8c-b6d3-d568f671af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(200, 4, 22, 250)\n",
    "b = np.random.randint(0, 3, (200, 1))\n",
    "c = np.random.rand(50, 4, 22, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c09167-e258-44a2-9c79-d3ca52113187",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = feature_extraction_4(a, b, c, sampling_freq=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6231c25-7553-4cd6-9dad-d34733d04434",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f54af-b544-49bc-bc85-d4c18ee1688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3498e-56fd-4a3c-9f9a-89d8d0f0cf2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Filter validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee4a442-77e3-40c2-9a4d-bbba6c642e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 4, 1000, False)  # 4 second\n",
    "sig = np.sin(2*np.pi*3*t) + 3 * np.sin(2*np.pi*10*t)\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(6, 10))\n",
    "ax1.plot(t, sig)\n",
    "ax1.set_title('Chebyshev type II -> sin(2pi x 3 x t) + 3 x sin(2pi x 10 x t)')\n",
    "ax1.axis([0, 1, -4, 4])\n",
    "\n",
    "sos1 = signal.cheby2(15, 20, [0.1, 4], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos1, sig)\n",
    "ax2.plot(t, filtered)\n",
    "ax2.set_title('0-4 band-pass filter')\n",
    "ax2.axis([0, 4, -3, 3])\n",
    "\n",
    "sos2 = signal.cheby2(10, 20, [4, 8], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos2, sig)\n",
    "ax3.plot(t, filtered)\n",
    "ax3.set_title('4-8 band-pass filter')\n",
    "ax3.axis([0, 4, -3, 3])\n",
    "\n",
    "sos3 = signal.cheby2(10, 20, [8, 12], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos3, sig)\n",
    "ax4.plot(t, filtered)\n",
    "ax4.set_title('8-12 band-pass filter')\n",
    "ax4.axis([0, 4, -3, 3])\n",
    "ax4.set_xlabel('Time [seconds]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccbe7a-025c-47f5-84c6-f5d5dc347080",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 4, 1000, False)  # 4 second\n",
    "sig = np.sin(2*np.pi*3*t) + 3 * np.sin(2*np.pi*10*t)\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(6, 10))\n",
    "ax1.plot(t, sig)\n",
    "ax1.set_title('FIR + Hamming - sin(2pi x 3 x t) + 3 x sin(2pi x 10 x t) ')\n",
    "ax1.axis([0, 1, -4, 4])\n",
    "\n",
    "\n",
    "filtered = mne.filter.filter_data(sig, 250, 0, 4)\n",
    "ax2.plot(t, filtered)\n",
    "ax2.set_title('0-4 band-pass filter')\n",
    "ax2.axis([0, 4, -3, 3])\n",
    "\n",
    "filtered = mne.filter.filter_data(sig, 250, 4, 8)\n",
    "ax3.plot(t, filtered)\n",
    "ax3.set_title('4-8 band-pass filter')\n",
    "ax3.axis([0, 4, -3, 3])\n",
    "\n",
    "filtered = mne.filter.filter_data(sig, 250, 8, 12)\n",
    "ax4.plot(t, filtered)\n",
    "ax4.set_title('8-12 band-pass filter')\n",
    "ax4.axis([0, 4, -3, 3])\n",
    "ax4.set_xlabel('Time [seconds]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f69a0-6ccd-4d96-b121-5c3784975ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 4, 1000, False)  # 4 second\n",
    "sig = np.sin(2*np.pi*3*t) + 3 * np.sin(2*np.pi*10*t)\n",
    "\n",
    "sig = np.tile(sig, (3, 1))\n",
    "\n",
    "print(sig.shape)\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, sharex=True, figsize=(6, 10))\n",
    "ax1.plot(t, sig[0])\n",
    "ax1.set_title('Chebyshev type II -> sin(2pi x 3 x t) + 3 x sin(2pi x 10 x t)')\n",
    "ax1.axis([0, 1, -4, 4])\n",
    "\n",
    "sos1 = signal.cheby2(15, 20, [0.1, 4], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos1, sig)\n",
    "ax2.plot(t, filtered[0])\n",
    "ax2.set_title('0-4 band-pass filter')\n",
    "ax2.axis([0, 4, -3, 3])\n",
    "\n",
    "sos2 = signal.cheby2(10, 20, [4, 8], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos2, sig)\n",
    "ax3.plot(t, filtered)\n",
    "ax3.set_title('4-8 band-pass filter')\n",
    "ax3.axis([0, 4, -3, 3])\n",
    "\n",
    "sos3 = signal.cheby2(10, 20, [8, 12], 'bp', fs=250, output='sos')\n",
    "filtered = signal.sosfilt(sos3, sig)\n",
    "ax4.plot(t, filtered)\n",
    "ax4.set_title('8-12 band-pass filter')\n",
    "ax4.axis([0, 4, -3, 3])\n",
    "ax4.set_xlabel('Time [seconds]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1f21a-0cfd-47f3-83bd-c8fce0b002ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# CSP Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a3584-b437-4721-8e7f-ec71c648e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 5, 1250, False)  # 4 second\n",
    "sig1 = np.sin(2*np.pi*35*t) + 3 * np.sin(2*np.pi*10*t)\n",
    "sig2 = np.sin(2*np.pi*100*t) + 3 * np.sin(2*np.pi*50*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcbd11c-b80c-466c-8d61-49f18aee6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t[:100], sig2[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201425d8-09a1-46cb-8d0c-378d7833f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(sig1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d19da9-7fbb-4a18-8755-06dec1eec22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig1 = np.expand_dims(sig1, axis=0)\n",
    "sig2 = np.expand_dims(sig2, axis=0)\n",
    "sig1.shape, sig2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5ce44-fff0-4229-85f9-b8403c669286",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = np.concatenate((sig1, sig2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285b81f-f669-429a-92af-64588e151cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = np.expand_dims(sig, axis=0)\n",
    "sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06892d0d-48f2-4f18-85a8-a90d72d90503",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_epoch = np.tile(sig, (50, 1, 1))\n",
    "s_epoch.shape, (s_epoch[0, :, :] == s_epoch[10, :, :]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054be730-6343-4aac-bca0-fb3f67605227",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train_epochs = slicer(s_epoch, num_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad68284-c289-4053-acf4-65bac5cadffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_train_epochs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17c7df-b3db-41c3-ab25-8fc8a426e902",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_no_encoded_train_labels = np.random.randint(0, 3, (50, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd6c71-a584-4cd3-ac33-b59b208b1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.set_log_level('warning')\n",
    "train_feats, test_feats, filtered_data = Preprocess_validator(s_train_epochs, s_no_encoded_train_labels, s_train_epochs, sampling_freq = 250, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ea1df-c4b4-4ca1-a02d-dfd1651bb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27141f70-c3f1-49ba-b28d-cd5e9d04b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad61840-6415-4f52-926a-a05de6e5e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(filtered_data[0, :, :, :] == filtered_data[10, :, :, :]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eed174-9ba8-4395-9816-c96c3b41022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(filtered_data[:, :, 0, :] == filtered_data[:, :, 0, :]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b19018-bb58-4663-8db5-501353dfeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "(filtered_data[:, 0, :, :] == filtered_data[:, 1, :, :]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08133fc2-1567-4d9c-9430-233936d15eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(s_train_epochs[:, 0, :, :] == s_train_epochs[:, 1, :, :]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07c24e-f100-4c01-8354-cad79144f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = filtered_data[0, 1, 0, :]\n",
    "plt.plot(t[:625], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1060ff4-6b2f-4699-9321-88e0f148343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = filtered_data[0, 0, 1, :]\n",
    "plt.plot(t[:625], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c18494-a9e6-4913-8d4c-f9872a2d0b84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Heatmap on FBCSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96757c4f-08e7-41fa-92a5-2abca36fa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_features_fortestID_[0].pickle\", \"rb\") as f:\n",
    "    train_features = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cb51e-534c-4448-b235-ec5093fb0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196148a-61da-48fb-b072-c5fab5b19e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.zeros((2160, 16, 80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82922b83-0aff-417f-b524-4ddb39628a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    new_array[:, :, 16*i:16*i+16] = train_features[:, i, :, :]\n",
    "    print((new_array[:, :, 16*i:16*i+16]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450dcd8-9a1e-4f55-a4fe-953d31c057d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616e6f6-6671-47a3-80db-a213afd2c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_value = new_array.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c498ac-b134-47d6-b478-f9e72d33cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heat_value, cmap='viridis', aspect='auto')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Heatmap for a 16x80 Matrix')\n",
    "\n",
    "# Show the colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08231c29-7a3a-420d-8bd5-239a9a2596f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epochs = np.concatenate(all_data_epochs)\n",
    "all_labels = np.concatenate(no_encode)\n",
    "\n",
    "\n",
    "mne.set_log_level('warning')\n",
    "all_features, _ = feature_extraction_cheby2(all_epochs, all_labels, all_epochs, sampling_freq = 250, n_components=16)\n",
    "#sys.stdout.close()\n",
    "#sys.stdout = original_stdout\n",
    "\n",
    "\n",
    "print(\"All epochs features shape:\", all_features.shape)\n",
    "\n",
    "\n",
    "with open(\"All_data_features.pickle\", \"wb\") as f:\n",
    "    pickle.dump(all_features, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee19ca-2da5-4847-8d53-4d1ac4fc4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.zeros((2448, 16, 80))\n",
    "for i in range(5):\n",
    "    new_array[:, :, 16*i:16*i+16] = all_features[:, i, :, :]\n",
    "    print((new_array[:, :, 16*i:16*i+16]).shape)\n",
    "print(\"\\n\\n\")\n",
    "print(\"new array shape:\", new_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59912349-f75a-45eb-b053-b024eea51294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All epochs features shape:\", all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc501f4d-2e04-47a0-a121-7b5e22610882",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_1 = new_array[(all_labels == 0).squeeze(), :, :]\n",
    "epochs_2 = new_array[(all_labels == 1).squeeze(), :, :]\n",
    "epochs_3 = new_array[(all_labels == 2).squeeze(), :, :]\n",
    "epochs_4 = new_array[(all_labels == 3).squeeze(), :, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d89aa-b596-4e11-8396-dda5e7576667",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Left epochs shape:  \", epochs_1.shape)\n",
    "print(\"Right epochs shape: \", epochs_2.shape)\n",
    "print(\"Feet epochs shape:  \", epochs_3.shape)\n",
    "print(\"Tongue epochs shape:\", epochs_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c27afb-4ee3-4695-90c9-d487744ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_1 = epochs_1.mean(axis=0)\n",
    "epochs_2 = epochs_2.mean(axis=0)\n",
    "epochs_3 = epochs_3.mean(axis=0)\n",
    "epochs_4 = epochs_4.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0031d6-3d2f-47f5-b006-1c79e1d00994",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586c46d-a472-46cb-ab10-6db16a914b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_labels == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b1c4c-8964-409d-84ab-afb6c06bb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, (8, 10))\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(epochs_1, cmap='viridis', aspect='auto')\n",
    "axs[0, 0].set_title(\"Left\")\n",
    "\n",
    "axs[0, 1].imshow(epochs_2, cmap='viridis', aspect='auto')\n",
    "axs[0, 1].set_title(\"Right\")\n",
    "\n",
    "axs[1, 0].imshow(epochs_3, cmap='viridis', aspect='auto')\n",
    "axs[1, 0].set_title(\"Feet\")\n",
    "\n",
    "axs[1, 1].imshow(epochs_4, cmap='viridis', aspect='auto')\n",
    "axs[1, 1].set_title(\"Tongue\")\n",
    "\n",
    "# Show the colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f46a9-1ed8-451f-9dfe-8cb1714ccd00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
